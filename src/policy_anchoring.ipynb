{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stable_baselines3 as sb3\n",
    "from stable_baselines3.ppo import PPO\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "from stable_baselines3.common.vec_env.dummy_vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from typing import Type, Optional, Union, Dict, Any, List, Tuple\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "from dynamic_environments import DynamicFrozenLakeEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyAnchoredPPO(PPO):\n",
    "    \"\"\"\n",
    "    Custom PPO class inheriting from Stable Baselines 3 PPO.\n",
    "    You can add custom logic for the algorithm by modifying this class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 policy: Union[str, Type[ActorCriticPolicy]], \n",
    "                 env: Union[DummyVecEnv, str], \n",
    "                 gp_threshold: float,  # Default reward threshold for a \"good\" policy\n",
    "                 gp_k: int = 5,        # Max number of good policies to store\n",
    "                 td_alpha: float = 0.5,       # Alpha value for task change detection\n",
    "                 learning_rate: Union[float, Any] = 3e-4,\n",
    "                 n_steps: int = 2048,\n",
    "                 batch_size: int = 64,\n",
    "                 n_epochs: int = 10,\n",
    "                 gamma: float = 0.99,\n",
    "                 gae_lambda: float = 0.95,\n",
    "                 clip_range: Union[float, Any] = 0.2,\n",
    "                 ent_coef: float = 0.0,\n",
    "                 vf_coef: float = 0.5,\n",
    "                 max_grad_norm: float = 0.5,\n",
    "                 verbose: int = 0,\n",
    "                 tensorboard_log: Optional[str] = None,\n",
    "                 _init_setup_model: bool = True,\n",
    "                 **kwargs):\n",
    "        \n",
    "        # Initialize the parent PPO class\n",
    "        super(PolicyAnchoredPPO, self).__init__(policy, env, learning_rate=learning_rate, n_steps=n_steps, \n",
    "                                        batch_size=batch_size, n_epochs=n_epochs, gamma=gamma,\n",
    "                                        gae_lambda=gae_lambda, clip_range=clip_range, ent_coef=ent_coef,\n",
    "                                        vf_coef=vf_coef, max_grad_norm=max_grad_norm,\n",
    "                                        verbose=verbose, tensorboard_log=tensorboard_log,\n",
    "                                        _init_setup_model=_init_setup_model, **kwargs)\n",
    "        \n",
    "        self.gp_threshold = gp_threshold  # Reward threshold to consider a policy as \"good\"\n",
    "        self.gp_k = gp_k                  # Max number of good policies to store\n",
    "        self.good_policies: List[Tuple[ActorCriticPolicy, float]] = []  # List of (policy, reward)\n",
    "        self.td_alpha = td_alpha  # Sensitivity parameter for task change detection\n",
    "        self.previous_rewards = []  # Buffer to store rewards of the previous training step\n",
    "\n",
    "\n",
    "    def update_good_policies(self, policy: ActorCriticPolicy, reward: float):\n",
    "        \"\"\"\n",
    "        Check if the policy is a 'good' policy based on the reward and update the list.\n",
    "        \"\"\"\n",
    "        if reward >= self.gp_threshold:\n",
    "            self.good_policies.append((policy, reward))\n",
    "\n",
    "            # Sort based on rewards in descending order and keep only top k policies\n",
    "            self.good_policies.sort(key=lambda x: x[1], reverse=True)\n",
    "            self.good_policies = self.good_policies[:self.gp_k]\n",
    "\n",
    "    def detect_task_change(self, current_rewards: List[float]):\n",
    "        \"\"\"\n",
    "        Detect if the task/environment has changed based on reward decline.\n",
    "        A task change is detected if the current rewards are significantly lower\n",
    "        than the mean of the previous rewards (by alpha times standard deviation).\n",
    "        \"\"\"\n",
    "        if len(self.previous_rewards) == 0:\n",
    "            # No previous rewards available to compare, skip detection\n",
    "            return\n",
    "        \n",
    "        # Calculate mean and standard deviation of previous rewards\n",
    "        prev_mean = np.mean(self.previous_rewards)\n",
    "        prev_std = np.std(self.previous_rewards)\n",
    "\n",
    "        # Calculate mean of current rewards\n",
    "        current_mean = np.mean(current_rewards)\n",
    "\n",
    "        # Check if there's a sharp decline (current rewards < prev_mean - alpha * prev_std)\n",
    "        if current_mean < (1 - self.td_alpha) * prev_mean  :\n",
    "            print(f\"Task change detected! Current rewards: {current_mean:.2f}, Previous mean: {prev_mean:.2f}, Std: {prev_std:.2f}\")\n",
    "\n",
    "    def _setup_model(self):\n",
    "        \"\"\"\n",
    "        Override to modify model setup if needed.\n",
    "        \"\"\"\n",
    "        super(PolicyAnchoredPPO, self)._setup_model()\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Overriding the train method to update the 'good' policies at the end of training iterations.\n",
    "        \"\"\"\n",
    "        super(PolicyAnchoredPPO, self).train()\n",
    "\n",
    "        # Retrieve accumulated rewards after the training step\n",
    "        if len(self.ep_info_buffer) == 0:\n",
    "            raise Warning(\"Cannot find any episode information in the buffer. Make sure you are using a VecEnv.\")\n",
    "        current_rewards = [ep_info['r'] for ep_info in self.ep_info_buffer]\n",
    "        accumulated_rewards = np.mean(current_rewards)\n",
    "\n",
    "        # Detect task change based on reward decline\n",
    "        self.detect_task_change(current_rewards)\n",
    "\n",
    "        # Store the current rewards for the next step comparison\n",
    "        self.previous_rewards = current_rewards\n",
    "\n",
    "        # Update the good policies based on current policy and accumulated rewards\n",
    "        self.update_good_policies(self.policy, accumulated_rewards)\n",
    "\n",
    "\n",
    "    def learn(self, total_timesteps: int, callback: Optional[BaseCallback] = None, \n",
    "              log_interval: int = 1, tb_log_name: str = \"PPO\", \n",
    "              reset_num_timesteps: bool = True, progress_bar: bool = False):\n",
    "        \"\"\"\n",
    "        Overriding the learn method to track good policies at the end of each iteration.\n",
    "        \"\"\"\n",
    "        return super(PolicyAnchoredPPO, self).learn(total_timesteps=total_timesteps, \n",
    "                                            callback=callback, \n",
    "                                            log_interval=log_interval, \n",
    "                                            tb_log_name=tb_log_name, \n",
    "                                            reset_num_timesteps=reset_num_timesteps,\n",
    "                                            progress_bar=progress_bar)\n",
    "    \n",
    "    def get_good_policies(self):\n",
    "        \"\"\"\n",
    "        Get the current list of good policies and their associated rewards.\n",
    "        \"\"\"\n",
    "        return self.good_policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.22     |\n",
      "|    ep_rew_mean     | 0.01     |\n",
      "| time/              |          |\n",
      "|    fps             | 448      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.98        |\n",
      "|    ep_rew_mean          | 0.02        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 358         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012494708 |\n",
      "|    clip_fraction        | 0.0435      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | -1.85       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00505     |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00797    |\n",
      "|    value_loss           | 0.00978     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.64        |\n",
      "|    ep_rew_mean          | 0.07        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 333         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009106711 |\n",
      "|    clip_fraction        | 0.0675      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | -0.0787     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0101     |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0165     |\n",
      "|    value_loss           | 0.00805     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.97        |\n",
      "|    ep_rew_mean          | 0.16        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 321         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015268853 |\n",
      "|    clip_fraction        | 0.247       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.33       |\n",
      "|    explained_variance   | 0.16        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.015      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0313     |\n",
      "|    value_loss           | 0.0377      |\n",
      "-----------------------------------------\n",
      "Switching environment map after 10000 steps.\n",
      "[[b'S' b'F' b'F' b'F' b'F' b'F' b'F' b'F']\n",
      " [b'F' b'F' b'F' b'F' b'F' b'F' b'F' b'F']\n",
      " [b'F' b'F' b'F' b'H' b'F' b'F' b'F' b'F']\n",
      " [b'F' b'F' b'F' b'F' b'H' b'F' b'F' b'F']\n",
      " [b'F' b'F' b'F' b'H' b'F' b'F' b'F' b'F']\n",
      " [b'F' b'H' b'H' b'F' b'F' b'F' b'H' b'F']\n",
      " [b'F' b'H' b'F' b'F' b'H' b'F' b'H' b'F']\n",
      " [b'F' b'F' b'F' b'H' b'F' b'F' b'F' b'G']]\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9.05        |\n",
      "|    ep_rew_mean          | 0.45        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 318         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 32          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019587371 |\n",
      "|    clip_fraction        | 0.325       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.24       |\n",
      "|    explained_variance   | 0.255       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0343     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0472     |\n",
      "|    value_loss           | 0.0856      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 8.26       |\n",
      "|    ep_rew_mean          | 0.59       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 314        |\n",
      "|    iterations           | 6          |\n",
      "|    time_elapsed         | 39         |\n",
      "|    total_timesteps      | 12288      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02006884 |\n",
      "|    clip_fraction        | 0.325      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.12      |\n",
      "|    explained_variance   | 0.252      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00832   |\n",
      "|    n_updates            | 50         |\n",
      "|    policy_gradient_loss | -0.0477    |\n",
      "|    value_loss           | 0.121      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.75        |\n",
      "|    ep_rew_mean          | 0.78        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 312         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 45          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024988716 |\n",
      "|    clip_fraction        | 0.246       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.964      |\n",
      "|    explained_variance   | 0.224       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0205     |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0441     |\n",
      "|    value_loss           | 0.119       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.02        |\n",
      "|    ep_rew_mean          | 0.85        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 311         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 52          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018563826 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.813      |\n",
      "|    explained_variance   | 0.0111      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00401     |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0268     |\n",
      "|    value_loss           | 0.069       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.PolicyAnchoredPPO at 0x7fadea6846b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = DynamicFrozenLakeEnv(switch_after=10000, is_slippery=False)\n",
    "env = make_vec_env(lambda: env, n_envs=1)\n",
    "\n",
    "model = PolicyAnchoredPPO(\"MlpPolicy\", env, gp_threshold= 1., verbose=1)\n",
    "\n",
    "model.learn(total_timesteps=15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DummyVecEnv' object has no attribute 'desc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munwrapped\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munwrapped\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdesc\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DummyVecEnv' object has no attribute 'desc'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = make_vec_env('CartPole-v1')\n",
    "\n",
    "# Create and train the custom PPO model\n",
    "model = PolicyAnchoredPPO(\"MlpPolicy\", env, gp_threshold= 200, verbose=1)\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "# Access the good policies\n",
    "good_policies = model.get_good_policies()\n",
    "print(f\"Good policies: {good_policies}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "obs = env.reset()\n",
    "env.render_mode = 'human'\n",
    "for _ in range(1000):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    time.sleep(0.01)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchwala",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
